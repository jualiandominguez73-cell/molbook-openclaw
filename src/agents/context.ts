// Lazy-load pi-coding-agent model metadata so we can infer context windows when
// the agent reports a model id. This includes custom models.json entries.

import { loadConfig } from "../config/config.js";
import { resolveOpenClawAgentDir } from "./agent-paths.js";
import { ensureOpenClawModelsJson } from "./models-config.js";
import { MODEL_CONTEXT_WINDOWS as OPENCODE_ZEN_CONTEXT_WINDOWS } from "./opencode-zen-models.js";

type ModelEntry = { id: string; contextWindow?: number };

const MODEL_CACHE = new Map<string, number>();
const loadPromise = (async () => {
  try {
    const { discoverAuthStorage, discoverModels } = await import("./pi-model-discovery.js");
    const cfg = loadConfig();
    await ensureOpenClawModelsJson(cfg);
    const agentDir = resolveOpenClawAgentDir();
    const authStorage = discoverAuthStorage(agentDir);
    const modelRegistry = discoverModels(authStorage, agentDir);
    const models = modelRegistry.getAll() as ModelEntry[];
    for (const m of models) {
      if (!m?.id) {
        continue;
      }
      if (typeof m.contextWindow === "number" && m.contextWindow > 0) {
        MODEL_CACHE.set(m.id, m.contextWindow);
      }
    }
  } catch {
    // If pi-ai isn't available, leave cache empty; lookup will fall back.
  }
})();

export function lookupContextTokens(modelId?: string): number | undefined {
  if (!modelId) {
    return undefined;
  }
  // Best-effort: kick off loading, but don't block.
  void loadPromise;

  // Dynamic cache first
  const cached = MODEL_CACHE.get(modelId);
  if (cached !== undefined) {
    return cached;
  }

  // Static fallback for known models (handles prefixed model IDs like "openai-codex/gpt-5.2")
  const bareModelId = modelId.includes("/") ? modelId.split("/").pop() : modelId;
  if (!bareModelId) {
    return undefined;
  }
  return OPENCODE_ZEN_CONTEXT_WINDOWS[bareModelId];
}
